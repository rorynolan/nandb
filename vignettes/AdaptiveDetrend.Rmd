---
title: "Adaptive Detrending"
author: "Rory Nolan"
date: "8 December 2016"
output: pdf_document
fontsize: 11pt
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, collapse = TRUE, comment = "#>")
set.seed(2016)
```

Load the packages:
```{r load packages, message=FALSE}
required.pkgs <- c("nandb", "EBImage", "autothresholdr", 
                   "tidyverse", "matrixStats", "KernSmooth")
invisible(sapply(required.pkgs, library, character.only = TRUE))
```

# Time-Series of Images with Bleaching

Let's start with an example.

```{r Read and display image}
eg.img <- system.file("extdata", "500.tif", package = "nandb") %>% 
  ReadImageData
display(normalize(eg.img[, , 1]), method = "raster")
```

Due to bleaching, the means of the images in the series decrease over time:
```{r real data frame means}
means <- apply(eg.img, 3, mean)
plot(means, ylim = c(13, 14.5), xlab = "frame", ylab = "mean")
```

Indeed we see some gentle bleaching here. This bleaching (if we don't correct for it) will increase the variance $\sigma^2$ in the intensity of a given pixel, and decrease the mean $\mu$ thereof; therefore it will (artificially) increase the calculated brightness $B = \frac{\sigma^2}{\mu}$. Digman et al. (2008) make the reasonable assumption that only immobile particles bleach (i.e. mobile particles move out of the illumination plane so quickly as to bleach negligibly compared to those that remain therein); we'll keep that assumption here. 

# Choosing the $\tau$ for the Bleaching Correction

Now I'll present three facts that might take a bit of time to get your head around:

1. Given that the bleaching is totally down to immobile particles, if we remove the contribution of mobile particles to the image, the amount of *bleaching* will not change.

1. If the contribution of mobile particles is removed, only immobile particles remain, so wherever there are particles, they are immobile and should (without the effect of bleaching) give brightness values of (on average) 1. 

1. Adding more immobile contribution to a pixel where the only contribution is immobile will increase the intensity, but the brightness will remain at (on average) 1.

Hence, if we simulate images with the same means as the experimental images, but having immobile particles replace the mobile ones (i.e. the simulated images are made using only immobile simulated particles), then we have the same amount of bleaching (measured by the change in the image means as in the plot above) but we have the added information that---after correcting for bleaching---the brightnesses of the pixels of this image series should have an average of 1. This is crucial information that we get from replacing mobile particles with immobile ones; before doing this, there was a mix of mobile and immobile and all we knew was that the average brightness should be some value greater than 1. 

For the simulated series, given that we know that aside from bleaching, it should have mean brightness 1, the choice of $\tau$ for the exponential filtering detrend should be the one which corrects this image series such that it has mean brightness 1.

# Simulating the Images

You wish to simulate $n$ frames of $p$ pixels. What is $p$? Say the the original images have $P$ pixels (e.g. $256 \times 256 = 65536$). If you threshold them, you exculde some pixels and are left with $p$ pixels where $p < P$ (the other pixels are set to \texttt{NA} by the thresholding). If you are not thresholding, then $p=P$. 

The various pixels of the image may have different amounts of immobile and mobile particles in them, and hence, when we replace the mobile particles with immobile, the pixels still won't necessarily have the *same* amount of immobile particles contributing to their intensity (and hence not the same expected intensity at each pixel). However, since we only intend to correct for bleaching on a whole-image scale,^[By *whole-image scale* I mean applying the same exponential filtering routine (same $\tau$) to each pixel in the series of frames, rather than having a different $\tau$ for each pixel. This is a good idea since at a global scale it is relatively easy to get a sense of bleaching whereas at the pixel microscale, it is hard to distinguish whether changes in intensity are due to particle movement or due to bleaching.] we are only really interested in the *average* (averaged across the whole image) bleaching of the image series.^[Inherent here is an assumption that the amount of bleaching per fluorophore is region-independent.]

Hence, the only criteria we want our simulated image series to have are:

1. They have the same number of pixels as the recorded frames (after thresholding).

1. They are simulated as having counts coming from immobile particle only.

1. The $i$^th^ frame in the simulated series has the same average intensity as the $i$^th^ frame in the real data. 

Here's how we satisfy them:

1. To find the number of pixels $p$, just count the number of non-\texttt{NA} pixels in the image post-thresholding, or if you don't threshold, it's just the number of pixels in the image.

1. Counts from stationary particles follow a Poission distribution, so each pixel intensity value should be drawn from a Poisson distribution with some mean $\lambda$.

1. Setting $\lambda$ equal to the desired mean for the whole frame (so a different $\lambda$ for each frame but the same $\lambda$ for each pixel within a given frame) will ensure that the whole frame will have mean $\lambda$ (or as near as makes no difference).^[This is common sense, but mathematically it comes from the central limit theorem.]

## The Image Simulation Algorithm

1. Count how many pixels $p$ you have in each frame (it's the same for each frame). 

1. Compute the means $\lambda_i$ of each frame in the real data.

1. For each frame $i$, draw $p$ times from a poisson distribution with mean $\lambda_i$. These are your pixel values for that frame. 

The the $j$^th^ pixel in simulated frame $i$ is the $j$^th^ poisson draw and you can carry on detrending and calculating brightness in the normal way. 

# Example

Let's apply this to our example image. Let's use triangle thresholding. 
```{r simulate the immobile images}
# Find p
p <- MedianPillars(eg.img) %>% round %>% auto_thresh_mask("tri") %>% sum
# Simulate the images
sims <- sapply(means, rpois, n = p)
```
So \texttt{sims} is a matrix where \texttt{sims[i, j]} is the \texttt{i}^th^ pixel in the \texttt{j}^th^ simulated frame. Before going on, let's just check that our simulations are as we would like:
```{r simulated frame means}
sim.frame.means <- colMeans(sims)
plot(sim.frame.means, xlab = "frame", ylab = "mean", ylim = c(13, 14.5))
```

Yes, this looks like the plot above, which is what we want. Now we can calculate the simulated brightnesses: 
```{r calculate simulated brightnesses}
sim.brightnesses <- rowVars(sims) / rowMeans(sims)
mean(sim.brightnesses)
```
So we see that it's not quite at 1. Let's try a detrend with $\tau = 10$:
```{r "check out tau=10"}
sims.tau10 <- sims - ExpSmoothRows(sims, 10) + 
  matrix(rowMeans(sims), nrow = nrow(sims), ncol = ncol(sims))
sim.brightnesses.tau10 <- rowVars(sims.tau10) / rowMeans(sims.tau10)
mean(sim.brightnesses.tau10)
```
So we see that $\tau = 10$ is far too severe. Let's go to the other extreme: $\tau = 500$:
```{r "check out tau = 500"}
sims.tau500 <- sims - ExpSmoothRows(sims, 500) + rowMeans(sims)
sim.brightnesses.tau500 <- rowVars(sims.tau500) / rowMeans(sims.tau500)
mean(sim.brightnesses.tau500)
```
And so we see that $\tau = 500$ is not severe enough. Hence we have lower and upper bounds on what $\tau$ should be. 

To find the $\tau$ which gives us a detrend that results in the simulated frames having mean brightness 1, we set up a function of tau which returns the distance of the mean brightness from 1 after detrending (with exponential filtering) with that parameter $\tau$. Then by minimizing this function with respect to $\tau$, we find the ideal $\tau$ that we seek.
First let's create that function:
```{r create the function for which we will find the root}
TauFarFromOne <- function(tau, sims) {
  sims.tau <- sims - ExpSmoothRows(sims, tau) + rowMeans(sims)
  brightnesses <- rowVars(sims.tau) / rowMeans(sims.tau)
  mean(brightnesses) - 1
}
```
Now minimize:
```{r find the ideal tau}
system.time(tau <- uniroot(TauFarFromOne, c(10, 500), sims, 
                           tol = 1, extendInt = "upX")$root)
tau
```
And thus we have the $\tau$ we should use.

This procedure is all wrapped up in a nice function:
```{r demonstrate the tau finding function from the package}
tau.again <- BestTau(eg.img, mst = "tri")
tau.again
```

Now you may be concerned that \texttt{tau} (`r round(tau, 2)`) and \texttt{tau.again} (`r round(tau.again, 2)`) are different. This is to be expected since there is a random element to the way they are computed: the images coming from all immobile particles are \emph{simulated}. Whilst the difference in \texttt{tau}s might seem alarming, they are actually relatively similar values in that detrends of \texttt{tau = }`r round(tau, 2)` and \texttt{tau.again = }`r round(tau.again, 2)` are actually relatively similar, indeed the difference between them is akin to the difference between \texttt{tau}s of `r 10` and `r round(10 * tau.again / tau, 2)`. Let's plot the brightnesses of no detrend, $\tau = 10$, `r round(tau, 2)` and `r round(tau.again, 2)` to see how similar they are.
```{r plot the brightnesses from the various detrends, fig.width=9}
GetDensityDF <- function(mat, detrend) {
  d <- bkde(mat[!is.na(mat)], range.x = c(0, 5), gridsize = 2^16)
  tibble(brightness = d$x, frequency = d$y, detrend = detrend)
}
brightnesses <- lapply(c(NA, 10, tau, tau.again), Brightness, mat3d = eg.img,
                       mst = "tri")
plot.df <- mapply(GetDensityDF, brightnesses, 
                  c("no detrend", "tau = 10", paste0("tau = ", round(tau, 2)),
                    paste0("tau = ", round(tau.again, 2))),
                  SIMPLIFY = FALSE) %>% 
  Reduce(rbind, .)
ggplot(plot.df, aes(brightness, frequency, colour = detrend)) + geom_line() + xlim(0, 5)
```
It seems that the brightnesses without detrend and those with detrends of $tau = `r tau`$ and tau = `r tau.again` roughly overlap, but the detrend of \texttt{tau = 10} has a significant shift.^[Recall that our calculations above show that a detrend with $\tau = 10$ is incorrect.] To check out the differences between no detrend, tau = `r tau` and tau = `r tau.again`, we'll have to zoom in:
```{r zoom in, fig.width=9}
ggplot(plot.df, aes(brightness, frequency, colour = detrend)) + 
  geom_line() + xlim(0.99, 1.01)
```
So this time we see that tau =  abc `r tau` and tau =  abc `r tau.again` are separated from no detrend, but to see the difference between them, we'll have to zoom in even further:
```{r zoom in further, fig.width=9}
ggplot(plot.df, aes(brightness, frequency, colour = detrend)) + 
  geom_line() + xlim(0.999, 1.001) + ylim(2.55, 2.65)
```
So one can begin to see that they are indeed different. The point is now well-made that by choosing tau = 10 we would have been way off, but using our procedure one can get a sensibly obtained value for tau and, although there is some uncertainty in its value to the randomness involved in simulating the images, this uncertainty and its effects are negligible.^[If one wanted to be extra-careful, one could run the procedure for estimating tau several times to get an idea of its distribution and then carry on taking into account the thus quantifiable uncertainty in tau.]
